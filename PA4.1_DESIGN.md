# PA4.1 Design – Leader-Based Replicated Server

## Goal

The goal in PA4.1 is to turn the single-server database into a replicated, *linearizable* datastore across multiple servers. The design uses a simple leader-based protocol so that all servers execute the same writes in the same order. [file:29][file:33]

---

## Main Idea: Single Leader, Ordered Writes

- All servers run the same code (`MyDBReplicatedServer`) but deterministically agree on one **leader** at startup.  
- Clients can send write requests to **any** server.  
- If a follower receives a write, it **forwards** the request to the leader.  
- The leader assigns a **monotonically increasing sequence number** to each write and broadcasts an **ORDER** message to all servers (including itself).  
- Each server stores the incoming ordered writes in a map and uses a lock plus a `nextToApply` counter to apply them **strictly in sequence order**. [file:29][file:33]

This pattern ensures every replica sees the same sequence of writes.

---

## Message Types and Formats

The system uses three types of messages/lines. [file:29]

### 1. Client → Server

- Format: either `requestId|CQL` or just `CQL`.  
- If the message does not contain a `|`, the server generates a random `requestId`.  
- Example:  
  - `123|insert into grade (id, events) values (1, [1,2,3]);`  
  - `insert into grade (id, events) values (1, [1,2,3]);` (requestId autogenerated)

Parsing logic is implemented in `MyDBReplicatedServer.handleMessageFromClient`. [file:29]

### 2. Follower → Leader (FORWARD)

- When a **non-leader** server receives a write, it forwards the request to the leader.  
- Format:  
  `FORWARD|requestId|base64(CQL)`  
- The CQL is Base64-encoded to safely transmit arbitrary characters. [file:29]

### 3. Leader → All Servers (ORDER)

- After assigning a sequence number, the leader broadcasts an ORDER message to all replicas.  
- Format:  
  `ORDER|seq|requestId|base64(CQL)`  
- `seq` is a strictly increasing long counter maintained only by the leader. [file:29][file:33]

---

## Leader Election

- At startup, each server obtains the list of node IDs from the `NodeConfig<String>` passed into `MyDBReplicatedServer`.  
- The server sorts the node IDs and chooses the **lexicographically smallest** ID as the leader (e.g., `server0` when IDs are `server0`, `server1`, `server2`).  
- This rule is deterministic and does not require any extra messages or voting. [file:29][file:33]

---

## Data Structures in MyDBReplicatedServer

Key fields: [file:29][file:33]

- `AtomicLong nextSeq` – sequence number generator used by the leader.  
- `Map<Long, Pending> pending` – maps sequence number → `Pending` struct.  
- `Pending` struct holds:
  - `String cql` – the CQL statement.  
  - `InetSocketAddress client` – client address (known only at the leader).  
  - `String requestId` – client’s request ID.  
- `long nextToApply` – the next sequence number that should be applied.  
- `Object applyLock` – monitor used to ensure only one thread applies requests at a time.  

These live in `MyDBReplicatedServer`. The class still extends `MyDBSingleServer`, which itself wraps the base `SingleServer`. [file:29][file:33]

---

## Protocol Flow

### 1. Client Request Handling

In `handleMessageFromClient`: [file:29]

1. Convert bytes → UTF-8 string and trim.  
2. Parse into `(requestId, cql)` using `split("\\|", 2)`.  
3. If `this` is the leader:
   - Call `onClientWriteAtLeader(requestId, cql, header.sndr)`.  
4. Otherwise (follower):
   - Call `sendForwardToLeader(requestId, cql, header.sndr)`.

### 2. Forwarding to Leader

`sendForwardToLeader`: [file:29]

- Encodes the CQL with Base64.  
- Builds `FORWARD|requestId|base64CQL`.  
- Sends it to the leader using `serverMessenger.send(leaderId, bytes)`.

### 3. Leader Handling of Forwarded Writes

`handleMessageFromServer` routes `FORWARD` messages to `onForwardLine`. [file:29]

- `onForwardLine` decodes the Base64 CQL and then calls the same method used for direct client writes:  
  `onClientWriteAtLeader(requestId, cql, header.sndr)`.

### 4. Leader Assigns ORDER and Broadcasts

`onClientWriteAtLeader`: [file:29]

1. Gets `seq = nextSeq.getAndIncrement()`.  
2. Stores the write in `pending.put(seq, new Pending(cql, clientAddr, requestId))`.  
3. Builds `ORDER|seq|requestId|base64CQL`.  
4. Multicasts this ORDER string to all servers via `multicastOrder`.

`multicastOrder` simply loops over all node IDs from `serverMessenger.getNodeConfig()` and calls `serverMessenger.send(node, bytes)`. [file:29]

---

## Applying Writes in Order

`onOrderLine` processes each ORDER: [file:29]

1. Parses `seq`, `requestId`, and Base64-decoded `cql`.  
2. Ensures there is a `Pending` entry for `seq`:
   - Follow­ers call `pending.putIfAbsent(seq, new Pending(cql, null, requestId))`.  
3. Calls `tryApplyInOrder()`.

`tryApplyInOrder` enforces sequential application: [file:29]

- Uses `synchronized (applyLock)` to allow only one thread inside at a time.  
- While there is an entry for `nextToApply`:
  - Retrieves the `Pending` object.  
  - Intended behavior (conceptually):
    - Execute the CQL against Cassandra (e.g., `session.execute(p.cql);`).  
    - If `p.client` is non-null (leader knows which client requested), send a response back via `clientMessenger.send(...)`.  
  - Removes the entry from `pending` and increments `nextToApply`.  

In the current code, the CQL execution is not fully wired into the Cassandra session yet, so `verifyOrderConsistent` fails, but the ordering and locking structure follow the intended design. [file:6][file:29]

---

## Why This Achieves Consistency (Conceptually)

- **Single sequencer:** Only the leader assigns sequence numbers, so each write gets a unique `seq`.  
- **Total ordering:** All replicas apply writes strictly in increasing `seq` order.  
- **Same input, same order:** Every server receives the same ORDER messages and applies them in the same sequence, so the final database state is identical across replicas (once CQL execution is wired). [file:29]

---

## Limitations / What PA4.1 Does Not Handle

- No fault tolerance: if the leader crashes, the system stalls; there is no leader re-election in PA4.1. [file:29][file:33]  
- No persistent log: ordering info is only in memory; restarting servers loses state.  
- Current code does not yet execute CQL on Cassandra in `tryApplyInOrder`, so the autograder’s data checks fail even though the protocol skeleton is implemented. [file:6]

These limitations are addressed conceptually in PA4.2 by adding Zookeeper logging and recovery.

